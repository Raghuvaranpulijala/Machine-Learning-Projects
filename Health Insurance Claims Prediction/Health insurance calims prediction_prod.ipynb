{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466940c1-8e83-4858-b35a-a38f3d7ea184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import logging\n",
    "from sklearn.impute import KNNImputer,SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import TargetEncoder\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,log_loss\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06850670-3275-4434-9469-850f70dd7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Health_insurance_claim_prediction:\n",
    "    def __init__(self,random_state=43):\n",
    "        self.preprocessor=None\n",
    "        self.model=None\n",
    "    def preprocess_data(self,df):\n",
    "        try:\n",
    "            print('preprocessing started')\n",
    "            df.drop(columns='ID',inplace=True)\n",
    "            numeric_features = list(df.select_dtypes(include=['int64','float64']).columns)\n",
    "            numeric_features = numeric_features[1:]\n",
    "            categorical_features = list(df.select_dtypes(include=['object']).columns)\n",
    "            numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer',KNNImputer(n_neighbors=5))\n",
    "            ])\n",
    "            categorical_transformer = Pipeline(steps=[\n",
    "                ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "                ('categorical encoding',TargetEncoder())\n",
    "            ])\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num',numeric_transformer,numeric_features),\n",
    "                    ('cat',categorical_transformer,categorical_features)\n",
    "                ])\n",
    "            X= df.drop('target',axis=1)\n",
    "            y=df['target']\n",
    "            features = list(X.columns)\n",
    "            print('transformation started')\n",
    "            X_transformed =preprocessor.fit_transform(X,y)\n",
    "            print('transformation ended')\n",
    "            X_transformed_df = pd.DataFrame(X_transformed,columns=features)\n",
    "            print('preprocessing ended')\n",
    "            return X_transformed_df,y\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "    def train_model(self,x,y):\n",
    "        try:\n",
    "            smote = SMOTE(sampling_strategy='minority',random_state=42)\n",
    "            x_resampled,y_resampled = smote.fit_resample(x,y.ravel())\n",
    "            x_train,x_test,y_train,y_test=train_test_split(x_resampled,y_resampled,random_state=23,test_size=0.3)\n",
    "            mlp_model = Sequential()\n",
    "            mlp_model.add(Dense(32,activation='relu',input_dim = x_train.shape[1]))\n",
    "            mlp_model.add(Dense(16,activation='relu'))\n",
    "            mlp_model.add(Dense(8,activation='relu'))\n",
    "            mlp_model.add(Dense(1,activation='sigmoid'))\n",
    "            mlp_model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "            history = mlp_model.fit(x_train,y_train,batch_size=500,epochs=100,verbose=0,validation_split=0.2)\n",
    "            print('model training started')\n",
    "            y_pred_probs = mlp_model.predict(x_test).flatten()\n",
    "            y_pred = mlp_model.predict(x_test)\n",
    "            y_pred = (y_pred > 0.5).astype(int) \n",
    "            accuracy = accuracy_score(y_test,y_pred)\n",
    "            precision_score1= precision_score(y_pred,y_test)\n",
    "            recall_score1 = recall_score(y_pred,y_test)\n",
    "            logloss = log_loss(y_test,y_pred_probs)\n",
    "            print('accuracy_score',accuracy)\n",
    "            print('precision',precision_score1)\n",
    "            print('recall',recall_score1)\n",
    "            print('logloss',logloss)\n",
    "            print('model training ended')\n",
    "            return mlp_model\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "                \n",
    "    def save_model(self,filepath):\n",
    "        try:\n",
    "            model_data = {\n",
    "                'model':self.model,\n",
    "                'preprocessor':self.preprocessor\n",
    "            }\n",
    "            joblib.dump(model_data,filepath)\n",
    "            print('model saved')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    " \n",
    "    def load_model(self,filepath):\n",
    "        try:\n",
    "            model_data = joblib.load(filepath)\n",
    "            self.model = model_data['model']\n",
    "            self.preprocessor = model_data['preprocessor']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    def predict(self,X):\n",
    "        try:\n",
    "            x_transformed = self.preprocessor.transform(X)\n",
    "            y_prob = self.model.predict(x_transformed)[:,1]\n",
    "            return y_prob\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f48e60a-b017-4958-a1be-38b13261842f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing started\n",
      "transformation started\n",
      "transformation ended\n",
      "preprocessing ended\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training started\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "accuracy_score 0.6925601750547046\n",
      "precision 0.600418410041841\n",
      "recall 0.7612732095490716\n",
      "logloss 0.568133153255109\n",
      "model training ended\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_data = pd.read_csv(r\"C:\\ML\\datasets\\bnp paribas\\train.csv\")\n",
    "    # train_data = train_data.iloc[:2000,:]\n",
    "    health_insurance_claim_system = Health_insurance_claim_prediction(random_state=43)\n",
    "    x,y = health_insurance_claim_system.preprocess_data(train_data)\n",
    "    health_insurance_claim_system.train_model(x,y)\n",
    "    # print('\\ncross_validation_results',cross_validation_results)\n",
    "    \n",
    "    health_insurance_claim_system.save_model('health_model.pkl')\n",
    "    # health_insurance_claim_system.load_model('health_model.pkl')\n",
    "    #sample_data = pd.read_csv(r\"C:\\ML\\datasets\\bnp paribas\\test.csv\")\n",
    "    #sample_data.drop('ID',inplace=True)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e87ab-4bd9-4cff-bff9-c7b571171203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
